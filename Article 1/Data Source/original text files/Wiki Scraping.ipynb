{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44560ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, requests\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import pycountry\n",
    "\n",
    "API_URL      = \"https://{lang}.wikipedia.org/w/api.php\"\n",
    "WD_API       = \"https://www.wikidata.org/w/api.php\"\n",
    "HEADERS      = {\"User-Agent\": \"MyWikiScraper/1.0\"}\n",
    "SLEEP        = 1e-5\n",
    "MAX_DEPTH    = 4\n",
    "OUTPUT_ROOT  = Path.cwd() / \"\"\n",
    "\n",
    "MAIN_LANG    = \"en\"\n",
    "MAIN_CAT     = \"\"\n",
    "\n",
    "visited = set()\n",
    "\n",
    "def sanitize(name: str) -> str:\n",
    "    return \"\".join(c if c.isalnum() or c in \" _-（）,.\" else \"_\" for c in name)\n",
    "\n",
    "def query_api(lang: str, params: dict, wd: bool=False) -> dict:\n",
    "    url = WD_API if wd else API_URL.format(lang=lang)\n",
    "    resp = requests.get(url, params=params, headers=HEADERS)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()\n",
    "\n",
    "def get_category_prefix(lang: str) -> str:\n",
    "    data = query_api(lang, {\n",
    "        \"action\":\"query\",\"format\":\"json\",\n",
    "        \"meta\":\"siteinfo\",\"siprop\":\"namespaces\"\n",
    "    })\n",
    "    ns14 = data[\"query\"][\"namespaces\"][\"14\"]\n",
    "    alias = ns14.get(\"alias\",\"\")\n",
    "    if not alias and \"aliases\" in ns14:\n",
    "        for a in ns14[\"aliases\"]:\n",
    "            if a.get(\"ns\")==14:\n",
    "                alias = a.get(\"*\",\"\")\n",
    "                break\n",
    "    return alias or ns14.get(\"*\",\"Category\")\n",
    "\n",
    "def get_category_members(lang: str, category: str, cmtype=\"page\") -> List[str]:\n",
    "    members = []\n",
    "    prefix = get_category_prefix(lang) + \":\"\n",
    "    params = {\n",
    "        \"action\":\"query\",\"format\":\"json\",\"list\":\"categorymembers\",\n",
    "        \"cmtitle\": prefix + category,\n",
    "        \"cmtype\": cmtype, \"cmlimit\":\"max\"\n",
    "    }\n",
    "    while True:\n",
    "        data = query_api(lang, params)\n",
    "        items = data[\"query\"][\"categorymembers\"]\n",
    "        if cmtype==\"subcat\":\n",
    "            members += [m[\"title\"].split(prefix,1)[-1] for m in items]\n",
    "        else:\n",
    "            members += [m[\"title\"] for m in items]\n",
    "        if \"continue\" in data:\n",
    "            params.update(data[\"continue\"])\n",
    "            time.sleep(SLEEP)\n",
    "        else:\n",
    "            break\n",
    "    return members\n",
    "\n",
    "def get_wikidata_sitelinks(qid: str) -> dict:\n",
    "    data = query_api(\"\", {\n",
    "        \"action\":\"wbgetentities\",\"ids\":qid,\n",
    "        \"props\":\"sitelinks\",\"format\":\"json\"\n",
    "    }, wd=True)\n",
    "    ent = data[\"entities\"][qid][\"sitelinks\"]\n",
    "    out = {}\n",
    "    for site,info in ent.items():\n",
    "        if not site.endswith(\"wiki\"):\n",
    "            continue\n",
    "        lc = site[:-4]\n",
    "        title = info[\"title\"]\n",
    "        if \":\" in title:\n",
    "            _, local = title.split(\":\",1)\n",
    "        else:\n",
    "            local = title\n",
    "        out[lc] = local\n",
    "    return out\n",
    "\n",
    "def fetch_extract(lang: str, title: str) -> str:\n",
    "    data = query_api(lang, {\n",
    "        \"action\":\"query\",\"format\":\"json\",\n",
    "        \"prop\":\"extracts\",\"explaintext\":True,\"titles\": title\n",
    "    })\n",
    "    page = next(iter(data[\"query\"][\"pages\"].values()))\n",
    "    return page.get(\"extract\",\"\")\n",
    "\n",
    "def get_category_qid(lang: str, category: str) -> str:\n",
    "    prefix = get_category_prefix(lang)\n",
    "    data = query_api(lang, {\n",
    "        \"action\":\"query\",\"format\":\"json\",\n",
    "        \"prop\":\"pageprops\",\"ppprop\":\"wikibase_item\",\n",
    "        \"titles\": f\"{prefix}:{category}\"\n",
    "    })\n",
    "    pages = data[\"query\"][\"pages\"]\n",
    "    qid = next(iter(pages.values()))[\"pageprops\"][\"wikibase_item\"]\n",
    "    return qid\n",
    "\n",
    "def scrape_category(lang: str, category: str, out_folder: Path, depth=1):\n",
    "    print(f\"{'  '*(depth-1)}Scraping [{lang}] Category: {category} (depth {depth})\")\n",
    "    out_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for title in get_category_members(lang, category, cmtype=\"page\"):\n",
    "        td = out_folder / sanitize(title)\n",
    "        td.mkdir(exist_ok=True)\n",
    "        variants = [(lang, title)]\n",
    "        ll = query_api(lang, {\n",
    "            \"action\":\"query\",\"format\":\"json\",\n",
    "            \"prop\":\"langlinks\",\"titles\": title,\"lllimit\":\"max\"\n",
    "        })\n",
    "        pid = next(iter(ll[\"query\"][\"pages\"]))\n",
    "        for llk in ll[\"query\"][\"pages\"][pid].get(\"langlinks\",[]):\n",
    "            variants.append((llk[\"lang\"], llk[\"*\"]))\n",
    "\n",
    "        for lang2, loc_title in variants:\n",
    "            key = (lang2, loc_title)\n",
    "            if key in visited:\n",
    "                continue\n",
    "            visited.add(key)\n",
    "            txt = fetch_extract(lang2, loc_title)\n",
    "            outp = td / f\"{sanitize(lang2)}.txt\"\n",
    "            outp.write_text(txt, encoding=\"utf-8\")\n",
    "            print(f\"{'  '*(depth-1)}  ✔ {lang2}: {loc_title}\")\n",
    "\n",
    "    if depth < MAX_DEPTH:\n",
    "        for sub in get_category_members(lang, category, cmtype=\"subcat\"):\n",
    "            scrape_category(lang, sub, out_folder/sanitize(sub), depth+1)\n",
    "\n",
    "\n",
    "def main():\n",
    "    OUTPUT_ROOT.mkdir(exist_ok=True)\n",
    "\n",
    "    qid = get_category_qid(MAIN_LANG, MAIN_CAT)\n",
    "    LANG_ROOTS = get_wikidata_sitelinks(qid)\n",
    "    LANG_ROOTS.pop(MAIN_LANG, None)\n",
    "    for lc, root_cat in LANG_ROOTS.items():\n",
    "        print(f\"\\n=== Processing [{lc}] Category: {root_cat} ===\")\n",
    "        folder = OUTPUT_ROOT / f\"{lc}_{sanitize(root_cat)}\"\n",
    "        folder.mkdir(parents=True, exist_ok=True)\n",
    "        for sub in get_category_members(lc, root_cat, cmtype=\"subcat\"):\n",
    "            scrape_category(lc, sub, folder/sanitize(sub), depth=1)\n",
    "\n",
    "    print(\"\\nAll done.\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
